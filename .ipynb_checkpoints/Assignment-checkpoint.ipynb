{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26770432-ab3d-4761-b9dc-864dae8527a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "# for stats tests\n",
    "import scipy.stats as st\n",
    "# for regression metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "# to find influential data points\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "# for diagnostic tests\n",
    "import statsmodels.stats.diagnostic as di\n",
    "import statsmodels.stats.stattools as stt\n",
    "# for general plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "# for the linear regression model and splitting data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn. discriminant_analysis import \\\n",
    "( LinearDiscriminantAnalysis as LDA ,\n",
    "QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn. naive_bayes import GaussianNB\n",
    "from sklearn. neighbors import KNeighborsClassifier\n",
    "from sklearn. preprocessing import StandardScaler\n",
    "from matplotlib .pyplot import subplots\n",
    "import statsmodels .api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ( ModelSpec as MS ,\n",
    "summarize )\n",
    "from sklearn. model_selection import train_test_split\n",
    "from sklearn. linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8a733-a9dc-44c5-9cc1-ce8b61697087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level = pd.read_csv('dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b12e1-d3dc-4f52-a913-14699e14c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2f95c-a462-4fa0-aa80-c6ab6a9f4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the size of the dataset\n",
    "df_sea_level.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91757a7-729b-4f0f-91ab-a49752a88bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the values for the columns\n",
    "df_sea_level['Indicator'].value_counts()\n",
    "df_sea_level['Source'].value_counts()\n",
    "df_sea_level['CTS Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c1190-5427-4947-a0f0-1ab81275cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the most importants columns for the study\n",
    "sea_levels = df_sea_level[['Indicator','Source','Measure','Date','Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada70b5-1fd4-4a6b-a380-951b27c4923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level.sort_values(by=['Date'], inplace=True)\n",
    "df_sea_level.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcb450-64c0-44e6-bb5b-d4a4e6c0652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case, the NAN values are not in a important column for the statistical or machine learning models, so, i will delete them\n",
    "#axis 1 its to specify that it is a column and all its to say all the records founded\n",
    "#Dont forget to add to the same dataframe the change\n",
    "df_sea_level = df_sea_level.dropna(axis=1, how = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a354215-01a3-4598-802f-b31ef055a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if all the NAn values was removed\n",
    "df_sea_level.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8472a-3a91-4ee5-9368-2918e79a3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is too much categories in the column of Measure, trying the inverse\n",
    "df_sea_level.plot.scatter(x=\"Value\", y=\"Measure\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478e1d0-6ff9-44c5-ba41-c65916554a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the Baltic Sea there is a big difference in the values, check for outliers\n",
    "outliers = df_sea_level['Value'].value_counts()\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbe90c-811c-4aea-9892-f06acbfff186",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df_sea_level['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919c9ea-568e-4b5e-8949-0c384e4ae01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplying Statistics tests, the previous box plot show that the values above more or less 200 are outliers\n",
    "\n",
    "upper_limit = df_sea_level['Value'].mean() + 3* df_sea_level['Value'].std() # Right from the mean\n",
    "lower_limit = df_sea_level['Value'].mean() - 3* df_sea_level['Value'].std() # Left from the mean\n",
    "print(upper_limit)\n",
    "print(lower_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada66b3b-6527-4940-a4a3-b80f61d0c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heck how many outliers are in the Value column with the help of the loc method.\n",
    "df_sea_level.loc[df_sea_level['Value'] >= upper_limit, 'Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38726153-3c3e-44b6-a4e2-75fa0276ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for outliers using the IQR\n",
    "IQR = df_sea_level['Value'].quantile(0.75) - df_sea_level['Value'].quantile(0.25)\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ace503-f506-4ba3-b93d-029d130b57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_sea_level['Value'], color=\"maroon\")\n",
    "plt.xlabel(\"Value\", labelpad=14)\n",
    "plt.ylabel(\"probability of occurence\", labelpad=14)\n",
    "plt.title(\"Distribution of Changing in the Sea Level in Millimeters\", y=1.015, fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8799e-a6a7-46a4-8ffe-4ab3f7212ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distplot its deprecated, so lets use other option\n",
    "sns.histplot(df_sea_level['Value'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c4e6b-5604-4c18-8504-f1735f2883a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the mean\n",
    "sea_level_mean = df_sea_level['Value'].mean()\n",
    "sea_level_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16bb56-1306-4625-8069-90709c4b5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Standard Deviation\n",
    "sea_level_std = df_sea_level['Value'].std()\n",
    "sea_level_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada8286-3723-4718-bd2b-7c836296d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addying Z-Score column in the dataset\n",
    "df_sea_level['Z-Score'] =(df_sea_level['Value']-sea_level_mean )/sea_level_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f76a96-d06b-4122-99d9-b41b6935895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883502a-5104-44f2-939d-3973fe479aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level['Z-Score'].hist(color='blue')\n",
    "plt.title(\"Standard Normal Distribution\", y=1.015, fontsize=22)\n",
    "plt.xlabel(\"z-score\", labelpad=14)\n",
    "plt.ylabel(\"frequency\", labelpad=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79655b6-8837-4683-9b70-1de86b947404",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_distribution_std_dev = round(df_sea_level['Z-Score'].std(), 2)\n",
    "z_score_distribution_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7bd91c-61b8-4a22-aa7a-2893de12f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level['Value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714d417-53e6-4860-996f-2d9ef8738c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sea_level['Measure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62797a6-1d33-4b7a-b5be-de16f4466fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Date isnt in the datetime format, so, we need to convert to datetime and do more analysis \n",
    "# Remove the 'D' and convert the 'Date' column to datetime format\n",
    "df_sea_level['Date'] = pd.to_datetime(df_sea_level['Date'].str[1:], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e254df-31f1-47f1-a078-841218a3c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "baltic_sea = df_sea_level[df_sea_level['Measure'] == 'Baltic Sea']\n",
    "baltic_sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bfa53-3527-48e9-922a-9dc79aaeecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baltic_sea = baltic_sea.sort_values(by='Date')\n",
    "baltic_sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34458be-d622-4994-840e-a26790b0109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Measure values over time for the Baltic Sea, the Baltic Sea had the marjority of the outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(baltic_sea['Date'], baltic_sea['Value'], label='Value', color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sea Level Measure Over Time for the Baltic Sea')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b585e89-4caf-457d-9dcf-9e6efefea62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the North Sea, in this article, say that the marjority of the changings in the Baltic Sea are influenced by the North Sea\n",
    "north_sea = df_sea_level[df_sea_level['Measure'] == 'North Sea']\n",
    "north_sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10ea5b-ece8-4629-a5e2-a6fcb955bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort by Date the North Sea\n",
    "north_sea = north_sea.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d89da-b2bb-4ca8-83fd-cd0fae1d8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Measure values over time for the North Sea\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(north_sea['Date'], north_sea['Value'], label='Value', color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sea Level Measure Over Time for the North Sea')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97d30f-c29e-451e-a986-6316b877bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now trying to put the two together, to better visualization\n",
    "# Plot the Measure values over time for the North Sea\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(baltic_sea['Date'], baltic_sea['Value'], label='Baltics Sea Values', color='blue')\n",
    "plt.plot(north_sea['Date'], north_sea['Value'], label='North Sea Values', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sea Level Measure Over Time for the North Sea and Baltic Sea')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa5908-7aaf-43da-a9bb-7c1b8a9441c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the correlation between them nortic sea values and the north sea values\n",
    "\n",
    "# Calculate the daily change in 'Measure' for each sea\n",
    "baltic_sea['Value_Change'] = baltic_sea['Value'].diff()\n",
    "north_sea['Value_Change'] = north_sea['Value'].diff()\n",
    "\n",
    "# Merge the two datasets on 'Date' to align the dates\n",
    "merged_data = pd.merge(baltic_sea[['Date', 'Value_Change']], \n",
    "                       north_sea[['Date', 'Value_Change']], \n",
    "                       on='Date', suffixes=('_Baltic', '_North'))\n",
    "\n",
    "# Calculate the correlation between the changes\n",
    "correlation = merged_data['Value_Change_Baltic'].corr(merged_data['Value_Change_North'])\n",
    "\n",
    "print(f\"Correlation between changes in the Baltic Sea and North Sea measures: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb92d4c-7a05-412a-bb09-108b3de5d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=merged_data, x='Value_Change_Baltic', y='Value_Change_North', color='blue')\n",
    "plt.title(f'Scatter Plot of Sea Level Changes\\nBaltic Sea vs North Sea\\nCorrelation: {correlation:.2f}')\n",
    "plt.xlabel('Baltic Sea Measure Change')\n",
    "plt.ylabel('North Sea Measure Change')\n",
    "plt.axhline(0, color='red', linestyle='--', lw=1)  # Add horizontal line at y=0\n",
    "plt.axvline(0, color='red', linestyle='--', lw=1)  # Add vertical line at x=0\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850cb86-76b2-45bf-a94a-26000f6524b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(baltic_sea['Date'], baltic_sea['Value_Change'], label='Baltic Sea Change', color='blue')\n",
    "plt.plot(north_sea['Date'], north_sea['Value_Change'], label='North Sea Change', color='orange')\n",
    "plt.title('Sea Level Changes Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Measure Change')\n",
    "plt.axhline(0, color='gray', linestyle='--', lw=1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a91336-233b-4023-9be2-49af5c62ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "# Create a DataFrame for correlation\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Baltic Sea Change': merged_data['Value_Change_Baltic'],\n",
    "    'North Sea Change': merged_data['Value_Change_North']\n",
    "})\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_df.corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Heatmap: Baltic Sea vs North Sea')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b67637-e0a4-422c-86f1-ae98fbd785a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create a wide format DataFrame\n",
    "pivot_df = df_sea_level.pivot_table(index='Date', columns='Measure', values='Value')\n",
    "\n",
    "# Calculate the correlation matrix for the sea values\n",
    "correlation_matrix = pivot_df.corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, \n",
    "            cbar_kws={\"shrink\": .8}, xticklabels=correlation_matrix.columns, \n",
    "            yticklabels=correlation_matrix.columns)\n",
    "plt.title('Multivariate Correlation Heatmap of Sea Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035bd59-5b09-4f6e-b150-5f386b1f464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create a wide format DataFrame\n",
    "pivot_df = df_sea_level.pivot_table(index='Date', columns='Measure', values='Value')\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb7e44-4da4-4226-a762-60d9d44ff75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = pivot_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df2304-9dd2-45d5-a972-e79f5334fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21791184-c59f-4289-a819-958375ad900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dependent variable (e.g., 'Baltic')\n",
    "y = pivot_df['Baltic Sea']\n",
    "\n",
    "# Select independent variables (all other seas)\n",
    "X = pivot_df.drop(columns='Baltic Sea')\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d2fab-81a1-4e25-a4e9-f355ade87c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Baltic Sea and other relevant seas\n",
    "seas_of_interest = ['Baltic Sea', 'North Sea']  # Add any other seas you want to analyze\n",
    "filtered_df = df_sea_level[df_sea_level['Measure'].isin(seas_of_interest)]\n",
    "\n",
    "# Check for NaNs in the filtered DataFrame\n",
    "print(\"Checking for NaNs in the filtered DataFrame:\")\n",
    "print(filtered_df.isna().sum())\n",
    "\n",
    "# Drop rows with NaN values in the 'Value' column\n",
    "filtered_df = filtered_df.dropna(subset=['Value'])\n",
    "\n",
    "# Check if there are any remaining NaN values after filtering\n",
    "print(\"Remaining NaNs after dropping:\")\n",
    "print(filtered_df.isna().sum())\n",
    "\n",
    "# Ensure the DataFrame contains only the seas of interest\n",
    "baltic_values = filtered_df[filtered_df['Measure'] == 'Baltic Sea'][['Date', 'Value']]\n",
    "north_values = filtered_df[filtered_df['Measure'] == 'North Sea'][['Date', 'Value']]\n",
    "\n",
    "# Merge the two DataFrames on the 'Date' column\n",
    "merged_df = pd.merge(baltic_values, north_values, on='Date', suffixes=('_Baltic', '_North'))\n",
    "\n",
    "# Drop any rows with NaN values after merging\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# Check if we have enough data for regression\n",
    "if merged_df.shape[0] < 2:\n",
    "    print(\"Not enough data for regression analysis.\")\n",
    "else:\n",
    "    # Prepare the dependent variable (Baltic Sea values)\n",
    "    y = merged_df['Value_Baltic']\n",
    "\n",
    "    # Prepare the independent variable (North Sea values)\n",
    "    X = merged_df['Value_North'].values  # Convert to numpy array for statsmodels\n",
    "    X = sm.add_constant(X)  # Add constant for the intercept\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Display the model summary\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d593abb-0b07-4084-8f79-b37fd8979bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the seas of interest\n",
    "seas_of_interest = [\n",
    "    'Southern Ocean', 'Indian Ocean', 'Nino', 'Atlantic Ocean',\n",
    "    'Tropics', 'North Pacific', 'Bering Sea', 'Pacific Ocean',\n",
    "    'Indonesian', 'Arabian Sea', 'South China', 'North Atlantic',\n",
    "    'Caribbean Sea', 'Baltic Sea', 'Gulf Mexico', 'Mediterranean',\n",
    "    'North Sea', 'Sea Okhotsk', 'Sea Japan', 'Bay Bengal',\n",
    "    'Yellow Sea', 'Andaman Sea', 'Adriatic Sea', 'Persian Gulf'\n",
    "]\n",
    "\n",
    "# Filter for the specified seas\n",
    "filtered_df = df_sea_level[df_sea_level['Measure'].isin(seas_of_interest)]\n",
    "\n",
    "# Sample 10% of the filtered DataFrame (you can adjust this fraction)\n",
    "sampled_df = filtered_df.sample(frac=0.1, random_state=1)  # random_state for reproducibility\n",
    "\n",
    "# Display the shape of the sampled DataFrame\n",
    "print(\"Sampled DataFrame shape:\", sampled_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ebb05-4b22-4a67-8a40-1dfccc929b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
